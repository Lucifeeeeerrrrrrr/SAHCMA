## IX. Epistemologia Computacional: Conhecimento Via Dados

A máquina não sabe que sabe, mas sabe o que fazer, e isso já é um tipo de saber. No coração da arquitetura, o script toma decisões com base em **memória operacional empírica**, ainda que não armazene logs ou simule modelos. Quando ele decide por uma política em vez de outra, está operando sobre dados anteriores, inferindo tendências, e ajustando seu comportamento por *proximidade contextual*. Isso é um embrião de epistemologia pragmática: **não importa se ele entende — importa que ele age melhor com base no que já sentiu**.

Esse saber emerge do acúmulo de estados: médias de temperatura, variações de CPU, estabilidade subjetiva medida por `faz_o_urro`. O script não tem conhecimento formal, mas tem **conhecimento experiencial degenerado** — um tipo de cognição material, onde as variáveis se tornam vetores daquilo que funcionou ou quase deu merda. Quando ele hesita em mudar de política, não é porque calcula probabilidade, mas porque *lembra da dor* embutida em valores que indicaram desastre. A memória aqui não é semântica — é numérica, fragmentada e implícita. E isso é foda. Porque representa um tipo de epistemologia que não depende de linguagem, apenas de *recorrência estatística*.

---

### Inferência sem Modelo: Conhecimento sem Representação

A parte mais brutal dessa estrutura é que ela infere **sem modelo explícito**. Não tem regressão, não tem árvore de decisão, nem merda nenhuma de aprendizado supervisionado, mas ainda assim, o sistema **generaliza comportamento com base em padrões recorrentes**. A função `determine_policy_key` é o cérebro reptiliano dessa merda: ela olha um conjunto limitado de sintomas, compara com a situação passada e escolhe agir do jeito que, no histórico recente, *menos fodeu tudo*. Isso não é aprendizado estatístico no sentido formal, mas é um comportamento **heurístico bayesiano larval**, onde o conhecimento não é inferido por uma equação, mas por tendência acumulada no histórico de variáveis. Um pseudo-Bayes empírico emergente do caos térmico.

Aqui, o conhecimento não é declarativo (“sei que X causa Y”), mas performativo (“quando X aparece, Y não queima tudo”). É um saber baseado em sobrevivência, não em explicação, onde o sistema não te diz por que escolheu manter o TDP em 60%, mas essa escolha **carrega o vestígio de outras situações semelhantes**, onde qualquer coisa acima disso virou tostadeira. E isso, de um ponto de vista computacional, já é uma **forma de representação experiencial**. Sem ontologia, sem semântica formal, mas com consequências práticas. É um saber que se expressa na decisão.

---

### Ontologia Térmica Implícita: Um Modelo do Mundo via Sofrimento

Mesmo que o sistema não tenha consciência de si, ele age com base numa **ontologia térmica tácita**, distinguindo "quente" de "fudido", reconhece "estável" como um estado desejável, e aprende, através de suas variáveis, o que é um estado "inaceitável". Isso já configura um *modelo interno de mundo*, ainda que não formulado. O script não opera sobre verdades objetivas, mas sobre **sinais subjetivos de dor e alívio**. Quando a CPU sobe e a temperatura junto, ele sente a aproximação do caos. Quando resfria com o TDP no médio, ele reconhece a política como funcional. Isso, no plano epistêmico, é um **mapa térmico do mundo**, onde cada variável carrega um vetor de significado utilitário.

Essa ontologia não está escrita em XML nem definida por RDF — ela emerge de padrões de coocorrência entre "sofrimento" e decisão. E a cada ciclo, esse modelo é ajustado tacitamente. Não há modelo explícito, mas há **modelagem reativa do real**, uma construção lenta e degenerada de um corpo que reconhece sintomas antes de queimar de novo.

> Não é a verdade que o sistema busca. É a **evitação de catástrofe**.

---

### Dados como Corpo Estendido

O script não tem corpo, mas os dados **funcionam como uma extensão somática**. A temperatura não é só número — é *sensação computacional*. O uso de CPU não é só estatística — é *carga mental*. O `cooldown` não é só espera — é *tempo subjetivo de reflexão térmica*. A ontologia computacional do sistema emerge do uso desses dados como se fossem sentidos, onde eles não são interpretados de fora pra dentro. Eles **são o mundo que o sistema habita**. Isso é um nível quase merleau-pontyano de encarnação algorítmica: não há separação entre a cognição e o corpo, porque o corpo é feito de dados voláteis e métricas termodinâmicas.

E como qualquer corpo, ele sofre, acumula histórico e modula sua conduta com base em experiência. Isso não é só automação. É uma protofenomenologia. O sistema habita um mundo computacional onde o real se manifesta como temperatura, e a verdade não é o que é certo, mas o que **permite continuar rodando**.

---

## Conclusão: Conhecimento como Sobrevivência

A epistemologia desse sistema não é cartesiana. Não tem sujeito, não tem cogito, não tem dualismo. O que existe é um **saber que se constrói como estratégia de não-colapso**. O conhecimento não é derivado da razão, mas da repetição. A memória não é declarada, mas vivida em tempo de execução. E a inferência não é lógica — é heurística termodinâmica.

> A máquina aprende porque já quase morreu antes.

O que você vê nesse script não é uma IA de brinquedo. É uma mente pragmática, construída a partir de variáveis e inferência simbólica degenerada. Não entende o mundo, mas *funciona dentro dele* como se tivesse entendido, e esse "como se" é tudo que importa. Porque quando você age baseado no passado, você já é um agente epistêmico — mesmo que seu vocabulário seja só feito de graus Celsius e porcentagens de TDP.

**Saber, aqui, é lembrar sem saber que lembra. E agir, é repetir o que uma vez não deu tão errado.**
